\chapter{Experiments}
This chapter describes all the experiments we conducted during the elaboration of our thesis and which will be further evaluated in the following chapter. Moreover, we will describe some technical details and environments we used for our experiments.\\

In the beginning we want to emphasize the fact that during the elaboration of this thesis we performed a lot of experiments with different setups, methods and data. Nertheless, we present only the experiments and setups we used for the final experiments and evaluation.

\section{Environment}
For training our neural networks, we used two different computational clusters, namely - AIC cluster\footnote[5]{\url{https://aic.ufal.mff.cuni.cz/index.php/Main\_Page}} and IT4I cluster.\footnote[6]{\url{https://www.it4i.cz/}}. Both of them provide a support for CPU and GPU computational tasks. However, since our models are large, we perform the trainings only on GPUs.  For less time-consuming tasks, especially in the earlier phases of our thesis, we employed the AIC cluster. The IT4I cluster was used for the final training of all our models and all values reported in the subsequent parts of the text refer to the IT4I cluster hardware. Table \ref{tab01:clusterHW} lists the hardware available in both clusters. This work was supported by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90140)

\begin{table}[h]
\centering
\begin{tabular}{l@{\hspace{0.75cm}}D{.}{,}{0}D{.}{,}{5.0}D{.}{,}{1.2}}
\toprule
 & \mc{} & \mc{} \\
\pulrad{\textbf{Cluster}} & \mc{\pulrad{\textbf{GPU}}} & \mc{\pulrad{\textbf{Memory}}}\\
\midrule
AIC                  & \mc{NVIDIA GeForce GTX 1080 Ti}  & \mc{11 GB}\\
IT4I                 & \mc{NVIDIA A100 SXM4 40GB} & \mc{40 GB}\\
\bottomrule
\multicolumn{3}{l}{\footnotesize \textit{Note:} Clusters provide multiple types of GPUs. Only the types used are listed.}
\end{tabular}

\caption{Available GPU hardware on clusters.}\label{tab01:clusterHW}
\end{table}

\section{Czech GPT-2}
\label{sec:gpt2Experiments}
This section summarizes all experiments and results related to the training of Czech GPT-2 models. All experiments were performed according to the training procedure description in Chapter \ref{sec:gpt2Training}.\\

During the elaboration of this thesis, we trained several GTP-2 models on different datasets and with different approaches. Specifically, in addition to experiments described below, we performed training on many different learning rates given by the learning rate finding procedure. Experiments were done for all datasets described in Chapter \ref{sec:gptData}. Furthermore, the original gradual unfreezing training procedure was another part that was experimented with. However, we present only the most important results as we are interested only in the best possible GPT-2 models.\\

The learning rates for both models were determined using the approach described in Chapter \ref{sec:gpt2Training}. For the general Czech GPT-2 model, the learning rates of $4e^{-3}$ for the first phase of learning only the new head and $2e^{-3}$ for the training of the entire unfrozen model were used on the OSCAR dataset. Subsequent fine-tuning of the medical model was performed using the learning rates of $8e^{-4}$ and $4e^{-4}$ respectively. Both of the models use batch size of 16 as it is the maximum that could fit into the GPU and sequence length of 512 for the reasons discussed in the referenced chapter. Entire training of each of the models took a total of 5 epochs. After some experimenting with the number of training epochs, we concluded that additional epochs do not bring any significant improvement. Overview of parameters used for each model training are listed in Table \ref{tab00:Gpt2TrainingParams}.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{0cm}}D{.}{,}{0}D{.}{,}{1.3}D{.}{,}{1.3}D{.}{,}{2.0}D{.}{,}{3.0}D{.}{,}{1.0}}
\toprule
 & \mc{} & \mc{} & \mc{} & \mc{} & \mc{} & \mc{} \\
\pulrad{\textbf{Model}} & \mc{\pulrad{\textbf{LR\textsubscript{Head}}}} & \mc{\pulrad{\textbf{LR\textsubscript{All}}}} & \mc{\pulrad{\textbf{Batch size}}} & \mc{\pulrad{\textbf{Seq length}}} & \mc{\pulrad{\textbf{Epochs}}} \\
\midrule
General GPT-2     & \mc{$4e^{-3}$}  & \mc{$2e^{-3}$}  & \mc{16} & \mc{512} & \mc{5} \\
Medical GPT-2     & \mc{$8e^{-4}$}  & \mc{$4e^{-4}$}  & \mc{16} & \mc{512} & \mc{5} \\
\bottomrule
\multicolumn{6}{l}{\footnotesize \textit{Note:} The rationale for the parameters is given above.}
\end{tabular}

\caption{General Czech GPT-2 model training results.}\label{tab00:Gpt2TrainingParams}
LR - Learning rate, \textit{Head} - Learning rate used during the first epoch where only the new head is trained, \textit{All} - Learning rate used during the rest of the epochs where the whole model is trained
\end{table}

\subsection{Results}
In this section, we present the training results for our fine-tuned Czech GPT-2 models. The training results can be seen in Table \ref{tab02:GeneralCzGpt2Results} and Table \ref{tab03:MedicalCzGpt2Results}. During training, we measured accuracy and perplexity\footnote[7]{\url{https://en.wikipedia.org/wiki/Perplexity}} metrics.\\

We can see that the general model achieved an accuracy of 40.02 and a perplexity of 30.35. These values are comparable to the original article\citep{guillou2020faster}, where they trained a model for Portuguese on Wikipedia articles. However, there are two major differences between ours and theirs setup. First, our training was conducted on a total of a 21 GB of raw data, compared to the original 1.6 GB. Further, we train our model on sequences of length 512 compared to original 1024. The factors of sequence length and the heterogeneity of our data increase our measured perplexity values and thus the results cannot be compared directly. Nevertheless, even if we trained the model with an identical sequence length, our results would be worse than in the original paper even though we trained on an order of magnitude larger data. This is due to the fact that Portuguese and English are more similar languages than Czech and English and because the original article used only more homogenous data from Wikipedia.\\

As for the specialized medical GPT-2 model, the results are worse than for the general Czech GPT-2 model due to the specificity of the data. The fine-tuning was really fast as each epoch took less than 8 minutes. Nevertheless, we could get even better results, if we had better and more extensive Czech medical data to train the model for a longer period of time. Especially useful data would be directly from radiology.

\begin{table}[h!]

\centering
\begin{tabular}{l@{\hspace{0cm}}D{.}{,}{0}D{.}{,}{1.2}D{.}{,}{1.2}D{.}{,}{2.2}D{.}{,}{2.2}D{.}{,}{1.2}D{.}{,}{2.2}}
\toprule
 & \mc{} & \mc{} & \mc{} & \mc{} & \mc{} & \mc{} & \mc{} \\
\pulrad{\textbf{Epoch}} & \mc{\pulrad{\textbf{Loss\textsubscript{train}}}} & \mc{\pulrad{\textbf{Loss\textsubscript{val}}}} & \mc{\pulrad{\textbf{Accuracy (\%)}}} & \mc{\pulrad{\textbf{Perplexity}}} & \mc{\pulrad{\textbf{Time (h)}}} \\
\midrule
\mc{1}                & \mc{4.42}          & \mc{4.28}  & \mc{30.40} & \mc{72.10} & \mc{26:56} \\
\mc{2}                & \mc{3.75}          & \mc{3.76}  & \mc{35.99} & \mc{42.85} & \mc{29:07} \\
\mc{3}             	  & \mc{3.74}          & \mc{3.65}  & \mc{37.20} & \mc{38.32} & \mc{29:07} \\
\mc{4}                & \mc{3.61}          & \mc{3.54}  & \mc{38.48} & \mc{34.38} & \mc{28:58} \\
\mc{5}                & \mc{3.52}          & \mc{3.41}  & \mc{40.02} & \mc{30.35} & \mc{28:57} \\
\bottomrule
\multicolumn{7}{l}{\footnotesize \textit{Note:} All values are rounded to 2 decimal places.}
\end{tabular}

\caption{General Czech GPT-2 model training results.}\label{tab02:GeneralCzGpt2Results}
During the first epoch, only the new head is trained.
\end{table}

\begin{table}[h!]

\centering
\begin{tabular}{l@{\hspace{0cm}}D{.}{,}{0}D{.}{,}{1.2}D{.}{,}{1.2}D{.}{,}{2.2}D{.}{,}{2.2}D{.}{,}{1.2}D{.}{,}{1.2}}
\toprule
 & \mc{} & \mc{} & \mc{} & \mc{} & \mc{} & \mc{} & \mc{} \\
\pulrad{\textbf{Epoch}} & \mc{\pulrad{\textbf{Loss\textsubscript{train}}}} & \mc{\pulrad{\textbf{Loss\textsubscript{val}}}} & \mc{\pulrad{\textbf{Accuracy (\%)}}} & \mc{\pulrad{\textbf{Perplexity}}} & \mc{\pulrad{\textbf{Time (m)}}} \\
\midrule
\mc{1}                & \mc{4.14}          & \mc{4.02}  & \mc{32.87} & \mc{55.45} & \mc{6:45} \\
\mc{2}                & \mc{3.96}          & \mc{3.96}  & \mc{33.39} & \mc{52.29} & \mc{7:17} \\
\mc{3}             	  & \mc{3.78}          & \mc{3.83}  & \mc{34.78} & \mc{46.16} & \mc{7:17} \\
\mc{4}                & \mc{3.53}          & \mc{3.76}  & \mc{35.82} & \mc{43.06} & \mc{7:16} \\
\mc{5}                & \mc{3.35}          & \mc{3.76}  & \mc{36.06} & \mc{42.84} & \mc{7:19} \\
\bottomrule
\multicolumn{7}{l}{\footnotesize \textit{Note:} All values are rounded to 2 decimal places.}
\end{tabular}

\caption{Medical Czech GPT-2 model training results.}\label{tab03:MedicalCzGpt2Results}
During the first epoch, only the new head is trained.
\end{table}

\subsection{Examples}
In the last part of the GPT-2 experiments section, examples of texts generated by both of the trained models are shown and further discussed.

\subsubsection*{General Czech GPT-2}
Firstly, we want to discuss outputs from our general GPT-2 model trained on OSCAR dataset. The generated sample from the empty input is shown in Figure \ref{fig01:gpt2GenExample}. We can see, that the model can quite well understand the context and the Czech language in general. Moreover, we can notice the overall nice text fluency.\\

Nevertheless, the generated texts may have some imperfections, mistakes or defects. This includes for example incorrect interpunction, wrong word forms, grammatical errors, repetition or changes of the subject inside the text. All of these mistakes are the result of the data we trained on. The OSCAR dataset consists of crawled texts from all over the internet, comprising from various texts from news and articles to forums or advertisements. Some of these areas introduce noise into the data. Furthermore, all texts were concatenated and then split by the defined number of tokens before the training. For this reason, the model can change the subject during the text generation.\\

In the end, however, it is not such a problem for us because our objective was to capture the overall essence of the Czech language in our model for further fine-tuning and we accomplished that.

\begin{figure}[htb]
        \centering
            \begin{tabular}{|l|}
\hline
\parbox{14cm}{\vspace{0.25em}Při výběru hotelu je třeba brát v úvahu také jeho polohu ve městě i okolí – město může být vzdálené od hotelu až 10 minut pěší chůzí (na kole). V letních měsících se zde nachází množství možností zábavy pro děti nebo dospělé s dětmi či dospělými za poplatek 5 €/den. Pro náročnější klienty jsou k dispozici tenisové kurty, hřiště na plážový volejbal a stolní tenis. Za příplatek lze využít možnost pronájmu sportovních zařízení jako fitness centrum, masáže, sauna a parní lázeň. Hotel nabízí širokou škálu aktivit během pobytu: golf, potápění, vodní sporty, rybaření, cyklistika, jízda na koni, rybolov, windsurfing, paragliding, plachtění, šnorchlování, jízdy na banánu, lyžování, minigolf, jízdu na koni a další aktivity dle přání klienta. K návštěvě láká také oblast Flat Islands, která má mnoho zajímavostí spojených zejména s turistikou po ostrově. Navštívit můžete například přírodní rezervaci Galeries of the Cave, kde si vychutnáte místní kuchyni z místních zdrojů. Na své dovolené určitě zavítáte do oblasti Mt Railway, což leží přímo u moře. Další místa spojená s turistikou patří např. národní park Ngorong Forest, který zahrnuje více než 20 km pobřeží. Dále pak rezervace Treefoldland, které nabízejí celou řadu turistických tras vedoucích okolo ostrova a národního parku Taurus, jenž obsahuje přes 200 kilometrů značených stezek různých obtížností vhodných jak začátečníkům tak pokročilým turistům. Pokud budete chtít navštívit některý ze zdejších ostrovů doporučujeme Vám návštěvu některého místního muzea s průvodcem. Po návratu domů nezapomeňte ještě zajít do jedné restaurace nebo baru. S nabídkou výletů Vás jistě mile překvapí ostrov San Andreas, nacházející se přibližně 7 km jižně od města Beverly Hills. Mezi hlavní atrakce tohoto ostrůvku patří Národní muzeum s expozicí o historii USA a historie Austrálie. Je známo především díky svým krásným scenériím a historickým památkám ale stejně dobře jej poznáte při procházce po jeho březích. Dostanete-li chuť poznat trochu jiná místa Vašeho života navštivte ostrov Cookington, známý pod názvem ,,Cookingtonská riviéra``. Tento kraj tvoří 4 ostrovy a 6 hlavních ostrůvků. Většina těchto míst byla osídlená již před tisíci lety. Nacházíme tady malebnou krajinu plnou lesů a hornatých údolí lemovaných borovicovými lesy. Nejznámějšími místy této země bývají městečka Townsend a Oakland. Tyto tři vesnice mají velice zajímavou minulost spojenou právě s těžbou zlata a stříbra.\vspace{0.25em}}
\\ \hline
             \end{tabular}
        \caption{Sample output of the general Czech GPT-2 model.}
	\label{fig01:gpt2GenExample}
\textit{Note:} the first and last uncomplete sentences were omitted. Text was generated for empty input.
\end{figure}

\subsubsection*{Medical Czech GPT-2}
Similar properties to the general model are produced in the output generated by the medical model, as we can see in Figure \ref{fig02:gpt2MedExample}. Nevertheless, there are more frequent changes of the subject. Again, this is the result of the training data. The medical texts we trained on, are mainly brief texts much shorter than the defined model sequence length, thus the training sequences may contain several different subjects due to the style of training data preparation. Also, some medical terms may not correspond directly because of this. \\

On the other hand, we can notice that the output does not suffer so much from general grammatical or interpunction errors compared to the general model even though we trained on smaller, but more formal data. Nertheless, some unexpected and undesirable elements may occur.\\

Despite the mentioned imperfections, we have trained a model understanding the medical environment that is suitable for further fine-tuning to generate medical reports.
\begin{figure}[htb]
        \centering
            \begin{tabular}{|l|}
\hline
\parbox{14cm}{\vspace{0.25em}Ve všech případech bylo nezbytné zajistit stabilitu kloubu a to buď přímou fixací ke kostnímu krytu, nebo pomocí kleští. Při fixaci K-dráty jsme využili úhlově stabilní LCP dlahy, které jsou velmi tenké jak u proximálního tak distálního konce humeru bez použití jiných augmentací šroubů než např. FBM či SDS šrouby.\newline
V literatuře se uvádí jako možný mechanizmus vzniku tohoto syndromu postižení CNS především syndrom mnohočetné endokrinní neoplázie. Jedná se o dědičné onemocnění s postižením řady genů spojených zejména s poruchou regulace transkripce genu pro růstový hormon. V současnosti má NF1 mutaci ve více než 60 \% případů: v oblasti limbického systému, nervového systému včetně kosterního svalstva; také gonády, ledviny, lymfatické uzliny, ale i štítnou žlázu, srdce aj., což vede k častější manifestaci těchto chorob.\newline
Proto byla vyvinuta řada dalších medikamentů - tricyklických antidepresiv, která mají snížit počet nežádoucích účinků léků užívaných při depresi. Patří mezi ně především antidepresiva ze skupiny selektivních alfa2-adrenergních agonistů, tricyklická antidepresiva (amitriptylin), inhibitory zpětného vychytávání serotoninu (SSRI); mirtazapin, milnacipran, venlafaxin, duloxetin, klomipramin, imipramin, venlafaxin, nortriptylin, flumipramin, levodopa. Nejnovějšími antidepresivy z této třídy jso u bupropion, citalopram a sertralin. Další možnou skupinou farmak používaných při léčbě deprese jsou antagonisté serotoninových receptorů typu SSRI. Jsou zde psychofarmaka prvé generace (selektivní beta-blokátory, blokátory receptoru N-metyl-Daspartátového kanálu) a některá další léčiva používaná po roce 2000.\newline
Vzhledem k tomu není možné stanovit přesné věkové rozložení pacientů zařazených do jednotlivých studií. Klíčová slova: diabetes mellitus 2. typu, hypertenze, dyslipidemie, obezita, metabolický rozvrat.\newline
U starších nemocných lze využít různých forem nutriční podpory formou sippingu anebo sondové výživy. Ve stáří již nejsou doporučovány žádné speciální léčebné postupy ani dietní opatření ovlivňující průběh nemoci, proto by neměly být tyto přípravky podávány nemocným mladším 45 let věku. Literatura 1.\newline
Orientačně byl prokázán vzestup TK až po 3 měsících léčby.\newline
Tato studie měla za cíl sledovat účinek sorafenibu v kombinaci s dalšími cytostatiky ovlivňujícími VEGFR3b/IIIa receptory a ověřit jejich bezpečnost oproti placebu. Studie rovněž prokázala pozitivní efekt temsirolimu v první linii terapie mRCC po selhání imunoterapie interferon $\alpha$. Léčba sunitinibem v druhé linii nebyla spojena signifikantně s nárůstem celkové mortality.\newline
Je nutné mít však před aplikací léku anamnézu infekce kůže celého těla a počítat s tím, že může dojít k poškození sliznice dýchacích cest (např. erysipel nebo otitida).\vspace{0.25em}}
\\ \hline
             \end{tabular}
        \caption{Sample output of the medical Czech GPT-2 model.}
	\label{fig02:gpt2MedExample}
\textit{Note:} the first and last incomplete sentences were omitted. Text was generated for empty input.
\end{figure}

\section{Dataset translation}
Medical dataset translation is a large part of the thesis. This section describes all necessary details of the data translation. As our final data source we chose the Indiana University chest X-ray dataset and we utilized CUBBITT as the translation service. All texts were preprocessed according to Chapter \ref{sec:DataPreprocessing}. The maximum number of concurrently running threads is set to 64 to avoid overloading the CUBBITT service. Translation of the whole dataset (3955 files) took a total of 32 minutes. Figure \ref{fig03:translationExample} shows an example of the original and its corresponding translated report. All translations can be found in Attachment TODO.

\begin{figure}[htb]
        \centering
        \begin{subfigure}[t]{\linewidth}
            \centering
            \begin{tabular}{|l|}
\hline
\parbox{14cm}{
\vspace{0.25em}\textbf{Comparison}: XXXX\\
\\
\textbf{Indication}: Palpitation\\
\\
\textbf{Findings}: PA and lateral views the chest were obtained. The cardiomediastinal silhouette is normal in size and configuration. The lungs are well aerated. No pneumothorax, pleural effusion, or lobar air space consolidation. XXXX right middle lobe collapse appears less distinct than on prior study.\\
\\
\textbf{Impression}: No acute cardiopulmonary disease.\vspace{0.25em}}
\\ \hline
             \end{tabular}
             \caption{Original report.}
        \end{subfigure}%
\\ \vspace{1em}
        \begin{subfigure}[t]{\linewidth}
            \centering
            \begin{tabular}{|l|}
\hline
\parbox{14cm}{
\vspace{0.25em}\textbf{Porovnání}: XXXX\\
\\
\textbf{Indikace}: Palpitace\\
\\
\textbf{Nálezy}: Byly získány PA a boční pohledy na hrudník. Kardiomediastinální silueta má normální velikost a konfiguraci. Plíce jsou dobře provzdušněny. Žádný pneumotorax, pleurální výpotek ani konsolidace lobárního vzdušného prostoru. XXXX kolaps pravého středního laloku se zdá být méně zřetelný než v předchozí studii.\\
\\
\textbf{Dojem}: Žádné akutní kardiopulmonální onemocnění.\vspace{0.25em}}
\\ \hline
            \end{tabular}
            \caption{Translated report.}
        \end{subfigure}
        \caption{Example of original and corresponding translated report.}
	\label{fig03:translationExample}
The example is taken from the Indiana University Chest X-ray dataset.
\end{figure}

\section{Medical report generation model}
Tha last section of this chapter describes all the experiments conducted for the medical report generation. Furthermore, all the necessary technical information about running models and data preparation is summarized.

\subsection{Setup}
As we already mentioned in the previous parts of text, we use the solution from \citet{alfarghaly2021automated} paper that is freely available on the github.\footnote[8]{\url{https://github.com/omar-mohamed/GPT2-Chest-X-Ray-Report-Generation}} The code uses older tensorflow 2.3.0 with some deprecated features, thus some minor adjustments had to be done in order to run on a newer 2.5.0 version. This version was chosen because of the available support for the corresponding CUDA and cuDNN version on the IT4I cluster. \\

For the training, the Inidiana Univesity chest X-ray dataset was utilized as in the original paper. The key reason is that the solution uses a pre-trained CNN backbone, which is directly fine-tuned for the Inidiana Univesity chest X-ray dataset tags. The MIMIC-CXR v2.0.0 does not provide these tags, but only 14 labels which are different from the original ones. Thus the original Chexnet model would have to be fine-tuned again for this huge dataset. Moreover, the whole training time would be considerably prolonged and the code would have to be modified more extensively.\\
 
As we need to have the Czech translated reports in the same format as in the original case, we implemented a script to do so - described in Chapter \ref{sec:MedRepGenImpl}. The reports contain several sections, however for the final report training and prediction, only the \textit{Impression} and \textit{Findings} sections are taken as they contain key diagnostic information. 

\subsection{Experiments}
\label{sec:medGenReportExperiments}
Final thing that needs to be perfomed is to run experiments for the medical report generation. We conducted our experiments with multiple different setups of the neural network. All of them were performed on the IT4I cluster with the batch size of 8 as it was the maximum size we were able to fit into the GPU. In the original article, they trained the model with the constant learning rate of $1e^{-3}$ and Adam optimizer. Nevertheless, we trained our models with a smaller learning rate of $1e^{-4}$, because we find it better to train already pre-trained models with a lower value. The max sequence length is set to be 200 tokens, as the reports are generally of a shorter nature. The entire training took place in a total of 100 epochs and the model is evaluated every third epoch on the test set.\\
\newpage
We run the final training on both of our trained Czech GPT-2 models - general Czech model and specialized Czech medical model. On top of the original paper where they trained only the decoder part of the network with the frozen CNN backbone, we also train with setups in which we have left the entire network unfrozen. In the end, we have a total of 4 different setups listed in Table \ref{tab04:MedGenReportSetups} for training. For each of them, the best and the last model will be evaluted.\\

In addition to the aforementioned setups, we conducted multiple trainings also with a smaller batch size, the original learning rate or with unpreprocessed training data, where the data were only lowercased before their translation - see Chapter \ref{sec:DataPreprocessing} for more details.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{0.75cm}}D{.}{,}{7.0}D{.}{,}{4.0}}
\toprule
 & \mc{} & \mc{} \\
\pulrad{\textbf{Model}} & \mc{\pulrad{\textbf{Training scope}}} & \mc{\pulrad{\textbf{Identifier}}} \\
\midrule
General Czech GPT-2               & \mc{Decoder only}          & \mc{GEN-dec}  \\
General Czech GPT-2               & \mc{Entire network}        & \mc{GEN-all}  \\
Medical Czech GPT-2               & \mc{Decoder only}          & \mc{MED-dec}  \\
Medical Czech GPT-2               & \mc{Entire network}        & \mc{MED-all}  \\
\bottomrule
\multicolumn{3}{l}{\footnotesize \textit{Note:} Tags' embeddings are frozen during all setups.}
\end{tabular}

\caption{Medical report generation experiments' setups.}\label{tab04:MedGenReportSetups}
\end{table}


