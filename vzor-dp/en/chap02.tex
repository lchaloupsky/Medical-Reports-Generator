\chapter{Our approach}
This chapter summarizes the overall design of our approach to generating textual medical reports for X-ray images. The problem consists of multiple independent parts we need to deal with - our approach in Chapter \ref{sec:ourApproach}, training Czech GPT-2 in Chapter \ref{sec:czechGpt2}, and translation of the English data in Chapter \ref{sec:medDataTranslation}. For each of them, we will present the fundamentals of our solution along with a description of related problematics and decisions made.

\section{Medical report generation}
\label{sec:ourApproach}
As we already mentioned in Chapter \ref{sec:RelatedWork}, the overall solution for the final medical report generation model is based on the \citet{alfarghaly2021automated} paper. We have chosen this approach for multiple reasons. The main reasons to use this work as the backbone for our thesis are the following:
\begin{enumerate}
	\item In the work, the state-of-the-art GPT-2 model is utilized as the language model. This gave us a great opportunity as there was no Czech GPT-2 model available at the time this thesis began.
	\item The encoder is already fine-tuned to extract visual features for a specific dataset.
	\item All solution source code is freely accessible on GitHub.\footnote[1]{\url{https://github.com/omar-mohamed/GPT2-Chest-X-Ray-Report-Generation}}
\end{enumerate}

As in most works for image captioning, the architecture is encoder-decoder based with the attention mechanism. The high-level solution architecture is depicted in Figure \hyperref[fig01:OmarArchitecutre]{2.1}.\\

The encoder part utilizes fine-tuned Chexnet\citep{rajpurkar2017chexnet} as its visual backbone. The Chexnet is a CNN Densenet121 model trained specifically for the medical environment on the ChestX-ray14\citep{wang2017chestx} dataset predicting 14 distinct disease classes from the input chest X-ray images. Input images are resized to the $224 \times 224$ resolution as in the case of the original Chexnet model. Moreover, the encoder is further fine-tuned to predict 105 of the most common manual tags from the Indiana University chest X-ray dataset (see Chapter \ref{sec:IUDataset}) for the purpose of a wider range of possible semantic features. The model thus produces two types of outputs - visual features from the base CNN model and class scores from the multi-label tag prediction.\\

In order to obtain semantic features from the predicted classes, each predicted class score is multiplied by its corresponding word2vec\citep{mikolov2013distributed} embedding, trained specifically on the biomedical texts, from the \citet{mcdonald2018deep}, and resulting in a weighted embedding matrix. Both the embedding matrix and visual features are further passed to the language model as the context vectors for the self-attention mechanism.\\

Instead of the original English GPT-2, the original solution used its smaller version distilGPT-2.\footnote[2]{\url{https://huggingface.co/distilgpt2}} The distilled version differs in the number of layers as it contains only a half of the layers compared to the original model and predicts only 512 tokens instead of 1024. Nevertheless, distilled versions of GPT-2 do not usually work well for languages other than English due to their reduced capacity. For these reasons, we use the original architecture of GPT-2 for further fine-tuning. But, our fine-tuned model will predict 512 tokens as well as the distilGPT-2 model, so we can compare it to a similar setup, reduce the training time, and use a larger batch size. During the medical report generation training, the maximal sequence length is set to 200 due to the fact that the reports in the dataset are shorter.\\
 
\begin{figure}[h]\centering
\includegraphics[width=145mm, height=57mm]{../img/OmarArchitecture}
\caption{Overall architecture used in our solution proposed in \citet{alfarghaly2021automated}.}
\label{fig01:OmarArchitecutre}
\end{figure}

\section{Czech GPT-2}
\label{sec:czechGpt2}
The aim of this work is to generate medical reports in the Czech language. In the previous parts, we decided to use GPT-2 as the language model. However, at the time of the beginning of this work, no Czech GPT-2 model was freely available and thus it was essentially necessary to create one. A Czech GPT-2 model has recently been published by \citet{hajek_horak2022}. This section describes all the steps needed for fine-tuning the small English GPT-2 to the Czech language. Respectively, we train two versions of the Czech GPT-2 model. One trained on general Czech textual data and one specialized specifically on Czech medical texts. The reason is that we have only a small amount of medical data, which is insufficient to directly train the medical Czech GPT-2 model. Instead, we apply a gradual approach, where we train the general Czech model on large data and then only fine-tune it on the medical data.

\subsection{Data}
\label{sec:gptData}
In this section we will describe the possible data applicable for the training of both the general and medical Czech GPT-2 model together with the decision made about the final data selection and data cleaning.

\subsubsection{General}
For the training of general Czech GPT-2 we have plenty of data options we can choose from. However, there are important properties of the data we need to satisfy. As we are transfer learning from English to the Czech language, we need to have sufficiently large data, so we ensure the GPT-2 will learn properly syntactic and semantic information. Moreover, the data have to be also heterogeneous enough, so the model can capture different types of information and not just, for example, newspaper articles from a specific area. In order to create a good enough general model, we need to meet these criteria.\\

Several different datasets were investigated and tested for the training of the general Czech GPT-2 model.
\paragraph*{Czech Wikipedia} ~\\
\indent The first data we used for training the model is the Czech~Wikipedia~dump.\footnote[3]{\url{https://dumps.wikimedia.org/cswiki/latest/cswiki-latest-pages-articles.xml.bz2}} After extraction, the total size of the dataset is approximately 800 MB of raw text. The advantages of this dataset are its easy accessibility and fairly clean data quality. On the other hand, the data are very homogeneous despite the various topics. Each article is written in the general descriptive style. Moreover, the data themself are not large enough, the trained model made many both syntactic and semantic mistakes during the text generation.

\paragraph*{Balanced Czech National Corpus} ~\\
\indent Another possibility was to use a balanced version of the Czech National Corpus\citep{11234/1-4635} as the original is composed mainly of journalistic articles. The balanced version\footnote[4]{We would like to thank Tomáš Musil for the preparation and balancing of the Czech National Corpus data.} tries to equalize the amount of data from each category. These categories include \textit{journalism}, \textit{poetry}, \textit{prose}, \textit{educational literature}, etc. The major advantage of this dataset is its purity, the texts are syntactically correct without any undesirable non-Czech elements and written in the standard Czech language. The dataset does not have any significant downsides and the trained model understood the Czech language without any significant ailments. In total, the dataset is comprised of 3,3 GB of raw text.

\paragraph*{OSCAR} ~\\
\indent OSCAR, from the \citet{ortiz-suarez-etal-2020-monolingual}, is a huge deduplicated multilingual corpus created from the Common~Crawl~corpus\footnote[5]{\url{https://commoncrawl.org/}} providing data for 166 different languages and available directly in the huggingface datasets library.\footnote[6]{\url{https://huggingface.co/datasets/oscar}} It consists of the text scraped from websites of very different kinds and thus the data are heterogeneous enough. Moreover, its huge size, as the Czech part of the dataset occupies a total of 24 GB of raw text, is another major benefit. On the other hand, because the data are automatically scraped, they carry a noise in them. Besides that, not negligible part of the text is in the non-standard Czech language as the data come from diverse web sources such as forums, etc. Nevertheless, the disadvantages are outweighed by the huge size of the corpus along with the following filtering of the text:
\begin{enumerate}
	\item We take only texts that are longer than 1200 characters as these texts tend to be longer articles written in the standard Czech language instead of advertisements, incomplete texts, etc.
	\item Any texts containing control characters are filtered out because the text contains generally undesirable content.
\end{enumerate}

\paragraph*{Conclusion} ~\\
\indent We analyzed various datasets along with their overall properties. Furthermore, the advantages and disadvantages of each were discussed. As a result, we have chosen the \textbf{OSCAR} dataset due to its size and heterogeneity. The \textbf{Wikipedia} dump is too small and homogeneous. On the other hand, the \textbf{Balanced Czech National Corpus} is heterogeneous enough, however, it is almost an order of magnitude smaller than \textbf{OSCAR}.

\subsubsection{Medical}
Since we will have a trained general Czech GPT-2 model from the previous section that already understands the Czech language, the final fine-tuning for the medical environment does not require that much data. We need to specialize the model to understand the medical environment inherently. For this purpose, we use a subset of the UFAL~Medical~Corpus~v.\ 1.0.\footnote[7]{\url{https://ufal.mff.cuni.cz/ufal\_medical\_corpus}} These data are further filtered to remove any inappropriate characters, lines, and redundant structures. As a result, the data contain a total of 100 MB of raw medical texts. The texts are comprised of general medical descriptions, articles, and package leaflets for medicines.

\subsection{Training}
\label{sec:gpt2Training}
In this part, we will describe the fine-tuning process of our Czech GPT-2 models. Both the general and the medical GPT-2 models are trained using the same process. The solution is based on the \citet{guillou2020faster} article using a specific fastai\footnote[8]{\url{https://www.fast.ai/}} library providing powerful tools using best practices aiming for training and fine-tuning neural network models. The reason why we used this article is that it has shown great results for transfer learning the English GPT-2 to Portuguese in a short time compared to traditional fine-tuning. The whole process consists of several important parts, that we will describe in the subsequent part of the text. Detailed information about the insides and experiments done are described in the following Chapter \ref{sec:gpt2Experiments}.

\subsubsection*{Training tokenizer}
First thing that needs to be done in the whole training process is to train a tokenizer specifically for the Czech language. As in the case of the original GPT-2 model, we use the same byte-level~byte-pair-encoding\citep{sennrich2015neural} tokenizer dividing input text into tokens (a word or its part). The original size of the vocabulary is kept and set to 50257, as well as the original special token $<|$endoftext$|>$ as an indication of the beginning/end of the sequence token and the pad token. The entire prepared datasets from Chapter \ref{sec:gptData} are used for training the tokenizers.

\subsubsection*{Initialization}
As we have indicated several times, for the GPT-2 model weights initialization we will use the weights from the original English GPT-2. However, we will do a modification of the initial word token embeddings.\\

For the word token embeddings, we will compare which tokens the English model tokenizer and our tokenizer have in common. The embeddings of these tokens will remain unchanged, as they have been already trained on a large text corpus and they already carry information, even though they come from a completely different language. The rest of the tokens will be initialized using the mean value of the English word token embeddings.

\subsubsection*{Data preparation}
Another step in the process is to prepare the dataset into a suitable structure for the training. The entire dataset is loaded and pre-tokenized in advance in order to reduce the data transfer time between the CPU and GPU needed. After the pre-tokenization process, all texts are further passed to a specific language modeling data object that is responsible for the preparation and handling of the training and validation data. All texts are concatenated, split by the defined sequence length, and formed into batches.\footnote[9]{The process may omit the $<|$endoftext$|>$ tokens and thus lose boundaries between documents. This can result in subject changes during text generation.} We use a batch size of 16 as it is the maximum size we were able to fit into the GPU and sequence length of 512 because we want to have a model corresponding to the one used in \citet{alfarghaly2021automated} - distilGPT2 - as our main goal is to generate medical reports, which are typically shorter than 512 tokens.\\

The batch size is one of the most important hyperparameters. In the following section, we will discuss, how learning rates are determined and also that we will use higher learning rates. Nevertheless, \citet{smith2018disciplined} recommends using the batch size as large as possible because higher learning rates are regularization themself and therefore other regularizations can be reduced, such as smaller batch size.

\subsubsection*{Finding optimal learning rate}
Fine-tuning is a very fragile process in terms of learning rates. The right choice of the learning rate is essential, if we choose a very small learning rate, the model will train slowly and will tend to overfit. On the other hand, if we choose too high learning rate, the whole process can diverge and all the progress will be lost.\\

\citet{smith2017cyclical} came up with a solution to this problem called \qq{LR range test}. Before the training itself, we do a pre-training run in which we are trying a wide range of learning rates for which we monitor their behaviour. Starting from very low learning rates up to very high learning rates, for every mini-batch, we try a current learning rate, collect the resulting loss, and move to the next iteration with a little higher learning rate. In the end, we plot collected losses against the corresponding learning rates.\\

An illustration of the result of this process can be seen in Figure \hyperref[fig02:lrFinder]{2.2}. The graph gives us an overview for which learning rates the model is still learning. The general recommendation is to use a learning rate that is an order of magnitude smaller than the minimum as the minimum is very close to the moment of divergence. We can also see in the graph that the current implementation gives us several other significant points usable for training.

\begin{figure}[h]\centering
\includegraphics[width=130mm, height=101mm]{../img/lrFinder}
\caption{Learning finder output.}
\label{fig02:lrFinder}
The \textit{Learning rate} axis is in logarithmic scale.
\end{figure}

\subsubsection*{Training process}
We have prepared everything we need in the previous parts. However, we still have to describe the essence of our fine-tuning process. We will first describe the learning rate schedule. Fine-tuning or transfer learning is usually done using small learning rates with decay, so the general information in the already pre-trained weights is kept while the model is specialized to a selected task. The disadvantage of this method is the long training time. Instead, we use a different approach from \citet{smith2018disciplined} called "1cycle" policy that is parametrized by \textit{minimal} and \textit{maximal} learning rates. In the beginning, we start with a \textit{minimal} learning rate and linearly increase it up to the \textit{maximum}. The second phase is then the opposite direction going down with cosine annealing, but instead of stopping at the \textit{minimum}, the decrease continues down by several orders of magnitudes lower as we can see in Figure \hyperref[fig03:lrSchedule]{2.3}. \\

The intuition behind this policy is quite natural. In the beginning, we start with a lower learning rate to find an optimal direction. As we are increasing the learning rate we are taking bigger steps in this direction, skipping sharp local minima and preferring wide flat local minima area as shown in the \citet{smith2019super}. The rest of the training is intended for improvement inside this area. This allows us to use higher learning rates and thus overall accelerate the entire training process. The \textit{maximal} learning rate should be chosen according to the previous section and the \textit{minimal} is defined as $ min = max / 25$ by default.\\

Another major part of the training are discriminative learning rates. This method has been proposed in \citet{howard2018universal}. Instead of training all layers at once, we assign a different learning rate for each layer or a group of layers for each of them. This arises from the reasoning about what the different parts of the network are focusing on. Therefore, we split our model into 4 distinct groups of layers and train each of them with a different learning rate. The assignment of different learning rates for each group is done directly inside the fastai library.\\

The fine-tuning process itself is divided into two parts: training the new head for the Czech language only and training the whole model at once. In the original article, they suggest a gradual unfreezing approach, where they unfreeze one more layer each time and train for one epoch. However, after numerous experiments, we observed that unfreezing the whole model and running multiple epochs at once gives better results.

\begin{figure}[h]\centering
\includegraphics[width=130mm, height=91mm]{../img/lrSchedule}
\caption{Learning rate schedule for 1cycle policy training.}
\label{fig03:lrSchedule}
The graph depicts the learning rate throughout the entire training process.
\end{figure}

\section{Medical dataset translation}
\label{sec:medDataTranslation}
For machine learning in general and NLP tasks especially, the data quality is the alpha and omega of the performance of the final model. Since, as we have already mentioned in Chapter \ref{sec:CzechData}, we do not have any Czech data directly for the medical examinations of X-rays, to obtain Czech data we must arrange ourselves in a different way. One potential way is to create a new artificial dataset using machine translation of existing datasets. This section discusses the required steps to build a quality dataset using translation.

\subsection{Translator choice}
In the previous part of the text, specifically in Chapter \ref{sec:Translators}, we already discussed several possibilities for automatic translation. Our final choice for the translator is CUBBITT as it provides REST API unlimited in the number of requests and volume.

\subsection{Preprocessing}
\label{sec:DataPreprocessing}
The most important part of our machine translation process is the preprocessing of the input text. We already outlined in Chapter \ref{sec:datasets} that the data contain some noise in them as we are dealing with reports in natural language. Moreover, we will use the CUBBITT translator, which does not perform auto-correction itself and cannot translate some patterns at all as we described in Chapter \ref{sec:Cubbitt}. For these purposes, we incorporate preprocessing before the translation as it would be beneficial to have all texts in the standardized form in order to firstly, help CUBBITT with translation to get the report correctly translated, and secondly to ensure that our model receives and processes all the data in an identical report format.\\

Our preprocessing pipeline encompasses of the following procedures. Some of them are dealing with general CUBBITT issues and others with specifics of the medical data.

\subsubsection*{Line starts}
First of all we start with a very simple procedure. We analyze all lines of the report and remove all whitespaces at the beginning and end of lines that are common to all lines. The purpose of this modification is to standardize the report format and get rid of unnecessary whitespaces while preserving its structure.

\subsubsection*{Anonymous sequences}
Inasmuch as the whole datasets are anonymized due to legal reasons and privacy protection, the reports contain \qq{anonymous sequences}, such as \qq{XXXX} or \qq{\underline{{ }{ }{ }{ }}}, denoting places with original private information about patients. However, these sequences can be attached to surrounding words or numbers forming undesirable words. As we already said, CUBBITT does not auto-correct its input automatically, so these inputs will not be handled in any special way and therefore could be translated incorrectly. For this reason, we separate these sequences to form independent words.

\begin{itemize}
	\item \qq{old man with left \underline{{ }{ }{ }{ }}rib} $\rightarrow$ \\ \qq{starý muž s levým \underline{{ }{ }{ }{ }}rib} (not translated \textit{rib})
	\item \qq{old man with left \underline{{ }{ }{ }{ }} rib} $\rightarrow$ \\ \qq{starý muž s levým \underline{{ }{ }{ }{ }} žebrem} (translated \textit{rib})
\end{itemize}

\subsubsection*{Units}
Subsequent form of correction that we perform is the separation of numbers and units attached to them. In addition, this also includes general cases, where the number and subsequent word are glued together while keeping specific medical terms with a similar structure. This is associated with the following step, as the units will not be true-cased properly without this procedure.

\begin{itemize}
	\item 10cm $\rightarrow$ 10 cm
	\item 3VD $\rightarrow$ 3VD (medical abbreviation)
\end{itemize}

\subsubsection*{True-casing}
The most important part of the whole preprocessing pipeline is the true-casing of the input text. This adjustment is necessary for two essential reasons. CUBBITT has problems with the translation of any uppercase texts in general. Capturing the true case of a text is a complex problem requiring either a large statistical language dictionary or a trained model in order to properly determine the case.\\

As medical reports are a very specific area, the existing solutions for general text true-casing are inapplicable. Medical reports contain a lot of abbreviations and acronyms, which can be often confused with ordinary English words. Training the model for medical true-casing requires even more specific data for a certain domain because in different contexts the common words can be treated differently. Moreover, obtaining flawless data to cover the entire specific domain is a challenging task. For these reasons, we chose the way of a statistical dictionary. Before the translation of the dataset begins, we create a statistical dictionary from the whole dataset containing the most often used form of every word. We also implement some additional rules for some exceptions, such as headings, that in some datasets can be in uppercase only.\\

Using the created dictionary, we deal with all uppercase words to assign them the proper form. The results of the true-casing preprocessing are demonstrated in the following examples, where \textit{1a} and \textit{2a} are translations without true-casing and \textit{1b} and \textit{2b} are translations with true-casing:
\begin{itemize}
	\item (1a) \qq{EXAMINATION: CHEST (PORTABLE AP)} $\rightarrow$ \\ \phantom{(1a)} \qq{PŘEZKOUŠENÍ: CHEST (PORTABLE AP)} \\ \phantom{(1a)} \textit{RE-EXAMINATION: CHEST (PORTABLE AP)}
	\item (1b) \qq{Examination: Chest (portable AP)} $\rightarrow$ \\ \phantom{(1b)} \qq{Vyšetření: Hrudník (přenosný~AP)} \\ \phantom{(1b)} \textit{Examination: Chest (portable AP)}
	\item (2a) \qq{SMALL RIGHT PLEURAL ABNORMALITY} $\rightarrow$ \\ \phantom{(2a)} \qq{MALÉ PRÁVO PLEURÁLNÍ ABNORMALITY} \\ \phantom{(2a)} \textit{SMALL LAW OF PLEURAL ABNORMALITY}
	\item (2b) \qq{Small right pleural abnormality} $\rightarrow$ \\ \phantom{(2b)} \qq{Malá pravá pleurální abnormalita} \\ \phantom{(2b)} \textit{Small right pleural abnormality}
\end{itemize}

\subsubsection*{Paragraphs structure}
In some of the medical reports the section headings and corresponding texts do not begin on the same lines. We adjust these situations to a form where each heading and the text belonging to it always starts on the same line, for two reasons. Firstly, we want to normalize the report structure in general and secondly, we want to move the section content as close as possible to the heading, so the translator and even the final model have the context close to each other.

\subsubsection*{Capitalization}
Another part of the preprocessing pipeline is a simple capitalization of each heading and each sentence in the report. This text capitalization process helps CUBBITT not only in the case of medical data but in general during the translation process of some texts to better understand the boundaries between sentences.

\subsubsection*{Time}
One of the patterns that CUBBITT does not recognize nor auto-correct itself in general are times. This applies both to the specification of hours and minutes, and to the  part of the day specification. If any of these parts are incorrectly formatted, the time will be translated incorrectly or not translated at all, and thus we would lose some information or it could damage the fluency of the translated text. For these reasons, we apply preprocessing to normalize all times. We can see the difference in the following examples:
\begin{itemize}
	\item \qq{Chest radiograph at 1045PM} $\rightarrow$ \qq{Rentgen hrudníku v 1045PM}
	\item \qq{Chest radiograph at 10:45 PM} $\rightarrow$ \qq{Rentgen hrudníku ve 22:45}
\end{itemize}

\subsubsection*{White spaces}
After all previous procedures, we apply one last very simple final modification, namely, we squash all the whitespace characters inside each line into a single space, while maintaining the format from the very first preprocessing step described above. This step is performed only for normalization purposes.

\subsubsection*{Lowercasing}
The last preprocessing procedure we will mention is the lowercasing of the whole text followed by capitalizing the first word of each sentence. This is a separate procedure that was used in the earlier phases of the elaboration of this work because CUBBITT has a problem with uppercased words as we already mentioned. This process has been replaced by the better true-casing process and is not used in the final version.















